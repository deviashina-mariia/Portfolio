{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели классификации комментариев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Описание проекта\n",
    "\n",
    "Интернет-магазин запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Требуется инструмент, который будет искать токсичные комментарии и отправлять их на модерацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### План проекта\n",
    "\n",
    "    1. Загрузка и подготовка данных\n",
    "    2. Обучение моделей и тестирование лучшей\n",
    "    3. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Описание данных\n",
    "\n",
    "В распоряжении набор данных с разметкой о токсичности правок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Загрузка и подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in d:\\anaconda\\envs\\ds_practicum_env\\lib\\site-packages (3.3.5)\n",
      "Requirement already satisfied: scipy in d:\\anaconda\\envs\\ds_practicum_env\\lib\\site-packages (from lightgbm) (1.8.0)\n",
      "Requirement already satisfied: wheel in d:\\anaconda\\envs\\ds_practicum_env\\lib\\site-packages (from lightgbm) (0.37.1)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in d:\\anaconda\\envs\\ds_practicum_env\\lib\\site-packages (from lightgbm) (0.24.1)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\envs\\ds_practicum_env\\lib\\site-packages (from lightgbm) (1.20.1)\n",
      "Requirement already satisfied: joblib>=0.11 in d:\\anaconda\\envs\\ds_practicum_env\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\anaconda\\envs\\ds_practicum_env\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (3.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Пользователь\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Пользователь\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Пользователь\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# загужаем необходимые библиотеки\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import nltk\n",
    "\n",
    "!pip install lightgbm\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для воспроизводимости результатов\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем данные\n",
    "try:\n",
    "    data = pd.read_csv('toxic_comments.csv', index_col=0)\n",
    "except:\n",
    "    data = pd.read_csv('/datasets/toxic_comments.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 159292 entries, 0 to 159450\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# выведем общую информацию:\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44375</th>\n",
       "      <td>\"With all of the TALK / \"\"lies\"\" that Chappell...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56397</th>\n",
       "      <td>Yeah I'm pretty much of a newbie here, actuall...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152370</th>\n",
       "      <td>I wonder if WikiProject The Isles? might work?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17221</th>\n",
       "      <td>ok i'll come back. But i get annoyed when ppl ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44097</th>\n",
       "      <td>mill dickwad admin]] = Hilarity ensues by bloc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61031</th>\n",
       "      <td>\":::I'm sure the Nazi scientists who performed...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54922</th>\n",
       "      <td>ha ha how are you gonna stop me?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91901</th>\n",
       "      <td>numerous evidence on the issue \\n\\nThe Himmler...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80693</th>\n",
       "      <td>\"\\n\\n Vad gör du? \\n\\nVarför raderar du text s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105996</th>\n",
       "      <td>I Could Sing Of Your Love Forever \\n\\nI Could ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic\n",
       "44375   \"With all of the TALK / \"\"lies\"\" that Chappell...      0\n",
       "56397   Yeah I'm pretty much of a newbie here, actuall...      0\n",
       "152370     I wonder if WikiProject The Isles? might work?      0\n",
       "17221   ok i'll come back. But i get annoyed when ppl ...      0\n",
       "44097   mill dickwad admin]] = Hilarity ensues by bloc...      0\n",
       "61031   \":::I'm sure the Nazi scientists who performed...      0\n",
       "54922                    ha ha how are you gonna stop me?      0\n",
       "91901   numerous evidence on the issue \\n\\nThe Himmler...      0\n",
       "80693   \"\\n\\n Vad gör du? \\n\\nVarför raderar du text s...      0\n",
       "105996  I Could Sing Of Your Love Forever \\n\\nI Could ...      0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# выведем 10 случайных строк для ознакомления:\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143106\n",
       "1     16186\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверка на сбалансированность классов:\n",
    "data['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#создаем функцию для регуляризации текстов:\n",
    "def texts_re(text):\n",
    "    text_re = re.sub(r'[^a-zA-Z\\']', ' ', text)\n",
    "    text_list = text_re.split()\n",
    "    return ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# регуляризация исходных данных\n",
    "corpus_re = data['text'].apply(lambda x: texts_re(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 1s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# токенизация исходных данных:\n",
    "#stemmer = PorterStemmer()\n",
    "tokens = corpus_re.apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для определения POS-тегов слов:\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем объект WordNetLemmatizer для лемматизации:\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для лемматизации текстов:\n",
    "def lemmatize_str(tokens_str):\n",
    "    lemmatize_tokens_str = []\n",
    "    for word in tokens_str:\n",
    "        if len(word)<35:\n",
    "            lemmatize_tokens_str.append(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
    "    return lemmatize_tokens_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1h 48min 23s\n",
      "Wall time: 1h 51min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# лемматизация исходных данных\n",
    "lemmatize_tokens = tokens.apply(lambda x: lemmatize_str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаем корпус лемматизированных текстов\n",
    "corpus = lemmatize_tokens.apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# приводим к нижнему регистру:\n",
    "corpus = corpus.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54090     the equation use in multidimensional newton ra...\n",
       "74813     pointing to a list that include blog and the w...\n",
       "52568     march utc it appear ron that you be frustratin...\n",
       "18614     threat the comment be delete for the reason i ...\n",
       "104865    hi there 'fruiters do you think file alfred ja...\n",
       "140344                   acceptable a far a anyone can know\n",
       "157703             it 's all explain very clearly at wp say\n",
       "32403     greek club stats somebody please provide a rel...\n",
       "1518      ripieno concerto a cinque two viola score one ...\n",
       "130778    followup ten edits from this ip to date includ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# выведем случайные 10 строк для просмотра результата очистки и лемматизации:\n",
    "corpus.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разделяем данные на обучающую и тестовую выборки:\n",
    "features_train, features_test, target_train, target_test = train_test_split(corpus, data['toxic'], test_size=0.25, \\\n",
    "                                                                            stratify=data['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем англоязычные стоп-слова\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Обучение моделей и тестирование лучшей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 Модель логистической регрессии:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 22min 3s\n",
      "Wall time: 21min 45s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[('tfidf',\n",
       "                                              TfidfVectorizer(stop_words={'a',\n",
       "                                                                          'about',\n",
       "                                                                          'above',\n",
       "                                                                          'after',\n",
       "                                                                          'again',\n",
       "                                                                          'against',\n",
       "                                                                          'ain',\n",
       "                                                                          'all',\n",
       "                                                                          'am',\n",
       "                                                                          'an',\n",
       "                                                                          'and',\n",
       "                                                                          'any',\n",
       "                                                                          'are',\n",
       "                                                                          'aren',\n",
       "                                                                          \"aren't\",\n",
       "                                                                          'as',\n",
       "                                                                          'at',\n",
       "                                                                          'be',\n",
       "                                                                          'because',\n",
       "                                                                          'been',\n",
       "                                                                          'before',\n",
       "                                                                          'being',\n",
       "                                                                          'below',\n",
       "                                                                          'between',\n",
       "                                                                          'both',\n",
       "                                                                          'but',\n",
       "                                                                          'by',\n",
       "                                                                          'can',\n",
       "                                                                          'couldn',\n",
       "                                                                          \"couldn't\", ...})),\n",
       "                                             ('model', LogisticRegression())]),\n",
       "                   param_distributions={'model__C': [0.1, 0.5, 1, 5, 10],\n",
       "                                        'tfidf__ngram_range': [(1, 1), (1, 2),\n",
       "                                                               (1, 4)]},\n",
       "                   scoring='f1')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# параметры логистической регрессии:\n",
    "params={'tfidf__ngram_range':[(1,1), (1,2), (1,4)], \\\n",
    "        'model__C':[.1,.5,1,5,10]} \n",
    "# создание pipeline для вычисления TF-IDF и модели логистической регрессии:\n",
    "pipe = Pipeline([ \n",
    "    ('tfidf', TfidfVectorizer(stop_words=stop_words)), \n",
    "    ('model',LogisticRegression())]) \n",
    "grid_linear = RandomizedSearchCV(pipe, params, cv=3, scoring='f1')\n",
    "grid_linear.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 Модель LGBMClassifier:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[('tfidf',\n",
       "                                              TfidfVectorizer(stop_words={'a',\n",
       "                                                                          'about',\n",
       "                                                                          'above',\n",
       "                                                                          'after',\n",
       "                                                                          'again',\n",
       "                                                                          'against',\n",
       "                                                                          'ain',\n",
       "                                                                          'all',\n",
       "                                                                          'am',\n",
       "                                                                          'an',\n",
       "                                                                          'and',\n",
       "                                                                          'any',\n",
       "                                                                          'are',\n",
       "                                                                          'aren',\n",
       "                                                                          \"aren't\",\n",
       "                                                                          'as',\n",
       "                                                                          'at',\n",
       "                                                                          'be',\n",
       "                                                                          'because',\n",
       "                                                                          'been',\n",
       "                                                                          'before',\n",
       "                                                                          'being',\n",
       "                                                                          'below',\n",
       "                                                                          'between',\n",
       "                                                                          'both',\n",
       "                                                                          'but',\n",
       "                                                                          'by',\n",
       "                                                                          'can',\n",
       "                                                                          'couldn',\n",
       "                                                                          \"couldn't\", ...})),\n",
       "                                             ('model', LGBMClassifier())]),\n",
       "                   param_distributions={'model__learning_rate': [0.01, 0.05,\n",
       "                                                                 0.1, 0.15],\n",
       "                                        'model__max_depth': [15, 20, 25, 30],\n",
       "                                        'tfidf__ngram_range': [(1, 1), (1, 4)]},\n",
       "                   scoring='f1')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# параметры TF-IDF и LGBMClassifier:\n",
    "params={'tfidf__ngram_range':[(1,1), (1,4)], \\\n",
    "        'model__max_depth': [15, 20, 25, 30], \\\n",
    "        'model__learning_rate':[.01,.05,.1,.15]} \n",
    "# создание pipeline для вычисления TF-IDF и модели LGBMClassifier:\n",
    "pipe = Pipeline([ \n",
    "    ('tfidf', TfidfVectorizer(stop_words=stop_words)), \n",
    "    ('model',LGBMClassifier())]) \n",
    "grid_lgbm = RandomizedSearchCV(pipe, params, cv=3, scoring='f1')\n",
    "grid_lgbm.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier</th>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    f1_score\n",
       "LogisticRegression      0.77\n",
       "LGBMClassifier          0.76"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# таблица с полученными оценками обученных моделей:\n",
    "criteria = pd.DataFrame(0, columns=['f1_score'], \\\n",
    "                        index=['LogisticRegression', 'LGBMClassifier'])\n",
    "criteria.loc['LogisticRegression', 'f1_score'] = round(grid_linear.best_score_, 2)\n",
    "criteria.loc['LGBMClassifier', 'f1_score'] = round(grid_lgbm.best_score_, 2)\n",
    "criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.772075055187638"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# расчет f1 лучшей модели на тестовой выборке:\n",
    "predicted_test = grid_linear.best_estimator_.predict(features_test)\n",
    "f1_score(target_test, predicted_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. В результате подготовки исходных данных:\n",
    "        - данные быди загружены, всего 159292 текстов\n",
    "        - произвели регуляризацию исходных текстов\n",
    "        - произвели токенизацию и лемматиацию исходных текстов\n",
    "        - разделили исходные данные на обучающую и тестовую выборки\n",
    "        - обучили счетчик для расчета TF-IDF на обучающей выборке\n",
    "        - произвели расчет TF-IDF для обучающей выборки\n",
    "    2. В результате обучения моделей:\n",
    "        - подобрали гиперпараметры для модели логистической регрессии\n",
    "        - подобрали гиперпараметры для модели случайного леса\n",
    "        - подобрали гиперпараметры для модели LGBMClassifier\n",
    "        - моделью с наилучшим значением метрики f1 является LGBMClassifier\n",
    "        - произвели расчет TF-IDF для тестовой выборки выборки\n",
    "        - произвели расчет f1 модели LGBMClassifier на тестовой выборке (f1=0.77)\n",
    "        - заданная точность достигнута"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
